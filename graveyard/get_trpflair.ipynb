{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "os.chdir(\"/Volumes/SAMSUNG/trpred\")\n",
    "\n",
    "# Get all trp users from submissions\n",
    "users_submissions = pd.read_csv(\"data/processed/submissions/TheRedPill-allsubmissions-2020-05-16-metadata.csv\", usecols = [\"id\",\"author\"])\n",
    "\n",
    "# Get submission ids to search for author flair by submission\n",
    "ls_subids = list(users_submissions.id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_id = \"993wvk\"\n",
    "test_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# https://api.pushshift.io/reddit/submission/search/?ids=9c7knc\n",
    "import requests \n",
    "\n",
    "url = \"https://api.pushshift.io/reddit/submission/search/?ids=\"\n",
    "url = url + test_id\n",
    "\n",
    "url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing this here so it doesn't restart every time i rerun it\n",
    "trpuser_flairs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import requests \n",
    "import time \n",
    "import pandas as pd\n",
    "import json\n",
    "import os \n",
    "\n",
    "os.chdir(\"/Volumes/SAMSUNG/trpred\")\n",
    "\n",
    "counter = 0\n",
    "filename = \"data/info/20201004_trp_use_flairs.json\"\n",
    "\n",
    "for i in ls_subids:\n",
    "    # set url to search by submission id\n",
    "    url = \"https://api.pushshift.io/reddit/submission/search/?ids=\"\n",
    "    url = url + i # concat id from list\n",
    "    r = requests.get(url) # make request\n",
    "    \n",
    "    author = users_submissions.loc[users_submissions['id'] == i]['author']\n",
    "    author = author.values[0]\n",
    "    \n",
    "    try:\n",
    "        flair = r.json()['data'][0]['author_flair_text'] # get flair if it exists\n",
    "    except:\n",
    "        flair = np.nan # otherwise, no flair\n",
    "    \n",
    "    trpuser_flairs[author] = flair\n",
    "    \n",
    "    print(i)\n",
    "    counter += 1 \n",
    "    print(\"og start +:\", counter)\n",
    "    \n",
    "    time.sleep(1)\n",
    "\n",
    "    if len(trpuser_flairs.keys()) % 1000 == 0:\n",
    "        try:\n",
    "            with open(filename, 'wb') as outfile:\n",
    "                json.dump(trpuser_flairs, outfile)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trpuser_flairs.keys())\n",
    "len(ls_subids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix up the DF\n",
    "df_flairs = pd.DataFrame.from_dict(subred_descriptions, orient = \"index\", columns = [\"flair\"])\n",
    "df_flairs = df_flairs.reset_index()\n",
    "df_flairs = df_flairs.rename(columns = {\"index\":\"user\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to csv\n",
    "os.chdir(\"/Volumes/SAMSUNG/trpred\")\n",
    "\n",
    "df_flairs.to_csv(\"info/trp_use_flairs.csv\", mode = \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "os.chdir(\"/Volumes/SAMSUNG/trpred\")\n",
    "\n",
    "# Get all trp users from comments\n",
    "users_comments = pd.read_csv(\"data/processed/comments/TheRedPill-comments-2020-06-01.csv\", usecols = [\"author\", \"author_flair_text\"])\n",
    "\n",
    "## NEED TO REPROCESS R/TRP COMMENTS BC OF STUPID ICLOUD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import utils\n",
    "import datetime as dt\n",
    "import csv\n",
    "import os\n",
    "import b_clean_data as clean\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from fsplit.filesplit import FileSplit\n",
    "import glob\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_files[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import utils\n",
    "import datetime as dt\n",
    "import csv\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from fsplit.filesplit import FileSplit\n",
    "import glob\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "def clean_comments(raw_comments):\n",
    "    \"\"\"Cleans comments found in the folder passed in.\"\"\"\n",
    "\n",
    "    new_folder = utils.create_folder(raw_comments)\n",
    "\n",
    "    if new_folder == None:\n",
    "        print(\"No directory created\")\n",
    "        new_folder = \"data/raw/comments/\" + utils.get_filename(raw_comments)\n",
    "\n",
    "    # Get name for processed file\n",
    "    print(\"raw_comments = \" + raw_comments)\n",
    "\n",
    "    \n",
    "    regex = r\"([^\\/]+)(?=.allcomments)\"\n",
    "    matches = re.search(regex, raw_comments)\n",
    "    new_file = matches.group(1)\n",
    "\n",
    "    # Comment files\n",
    "    files = [f for f in listdir(new_folder) if isfile(join(new_folder, f))]\n",
    "    print(*files, sep = \"\\n\")\n",
    "    \n",
    "    # Create list of columns to keep\n",
    "    keep_cols = ['id', 'created_utc','author',\\\n",
    "                  'author_flair_text', 'score', 'parent_id',\\\n",
    "                  'subreddit']\n",
    "    keep_cols_text = ['id', 'created_utc', 'parent_id', 'body']\n",
    "\n",
    "    # Create file name\n",
    "    processedfile_csv = \"data/processed/comments/\" + new_file + \\\n",
    "        \"-metadata\" +  \".csv\"\n",
    "\n",
    "    processed_textfile_csv = \"data/processed/comments/\" + new_file + \\\n",
    "        \"-text\" + \".csv\"\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    # Read in json file\n",
    "    for i in files:\n",
    "        print(i)\n",
    "        counter += 1\n",
    "\n",
    "        df_keep = pd.DataFrame()\n",
    "        df_keep_text = pd.DataFrame()\n",
    "\n",
    "        file_path = new_folder + \"/\" + i\n",
    "\n",
    "        try:\n",
    "            data = pd.read_json(file_path)\n",
    "\n",
    "        # ValueError: Trailing data thrown if file is pretty indented\n",
    "        except ValueError:\n",
    "            data = pd.read_json(file_path, lines = True)\n",
    "\n",
    "\n",
    "        try:\n",
    "            df_keep = df_keep.append(data[keep_cols])\n",
    "        except KeyError:\n",
    "            keep_cols = ['id', 'created_utc', 'author', 'title',\\\n",
    "                        'score', 'num_comments', 'subreddit']\n",
    "            df_keep = df_keep.append(data[keep_cols])\n",
    "\n",
    "\n",
    "        try:\n",
    "            df_keep_text = df_keep_text.append(data[keep_cols_text])\n",
    "        except KeyError:\n",
    "            keep_cols_text = ['id', 'created_utc', 'author']\n",
    "            df_keep_text = df_keep_text.append(data[keep_cols_text])\n",
    "\n",
    "        # Make sure there's at least 1 observation\n",
    "        observations = len(df_keep.index)\n",
    "\n",
    "        # Change date format\n",
    "        ## For metadata\n",
    "        if observations == 0:\n",
    "            print(\"No comments found in \" + i)\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            df_keep['datetime_dv'] = pd.to_datetime(df_keep['created_utc'], unit = 's')# dv = derived\n",
    "            df_keep['date_dv'] = df_keep['datetime_dv'].dt.date\n",
    "\n",
    "            # For text\n",
    "            df_keep_text['datetime_dv'] = pd.to_datetime(df_keep_text['created_utc'], unit = 's')# dv = derived\n",
    "            df_keep_text['date_dv'] = df_keep_text['datetime_dv'].dt.date\n",
    "\n",
    "\n",
    "        ##### Delimit by date #####\n",
    "        start = dt.datetime.fromtimestamp(utils.get_startdate()).date()\n",
    "        end = dt.datetime.fromtimestamp(utils.get_enddate()).date()\n",
    "\n",
    "        # Create mask of time slot\n",
    "        mask = (df_keep['date_dv'] >= start) & (df_keep['date_dv'] <= end) # inclusive on either end\n",
    "        mask_text = (df_keep_text['date_dv'] >= start) & (df_keep_text['date_dv'] <= end)\n",
    "\n",
    "        # Only keep data within date frame\n",
    "        df_keep = df_keep.loc[mask]\n",
    "        df_keep_text = df_keep_text.loc[mask_text]\n",
    "        ############################\n",
    "\n",
    "        # Save to csv\n",
    "        if counter == 1:\n",
    "            df_keep.to_csv(processedfile_csv, mode = \"w\")\n",
    "            df_keep_text.to_csv(processed_textfile_csv, mode = \"w\")\n",
    "\n",
    "        else:\n",
    "            df_keep.to_csv(processedfile_csv, mode = \"a\", header = False)\n",
    "            df_keep_text.to_csv(processed_textfile_csv, mode = \"a\", header = False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clean comments and submissions\n",
    "os.chdir(\"/Volumes/SAMSUNG/trpred\")# Get comment folders\n",
    "\n",
    "# comment_folders = [x[0] for x in os.walk(\"data/raw/comments\")]\n",
    "# comment_folders = comment_folders[1:] # item at index 0 is parent folder so exclude\n",
    "\n",
    "comment_files = glob.glob(\"data/raw/comments/*.json\")\n",
    "\n",
    "\n",
    "# comment_files = [\"data/raw/comments/Braincels-allcomments-2020-10-02.json\",\n",
    "# \"data/raw/comments/exredpill-allcomments-2020-10-03.json\",\n",
    "# \"data/raw/comments/fPUA-allcomments-2020-10-03.json\",\n",
    "# \"data/raw/comments/IncelTears-allcomments-2020-10-03.json\",\n",
    "# \"data/raw/comments/KotakuInAction-allcomments-2020-10-06.json\",\n",
    "# \"data/raw/comments/masculism-allcomments-2020-10-03.json\",\n",
    "# \"data/raw/comments/MensRights-allcomments-2020-10-02.json\",\n",
    "# \"data/raw/comments/MGTOW-allcomments-2020-10-03.json\",\n",
    "# \"data/raw/comments/PurplePillDebate-allcomments-2020-10-03.json\",\n",
    "# \"data/raw/comments/TheRedPill-allcomments-2020-10-01.json\"]\n",
    "\n",
    "\n",
    "for i in comment_files:\n",
    "    print(i)\n",
    "    clean_comments(i)\n",
    "    print(i + \" end\")\n",
    "    \n",
    "    \n",
    "### NEW FILE ISNT BEING CREATED -- CHECK WHY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comment_files[4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"data/raw/comments/KotakuInAction_allcomments_10-06.json\"\n",
    "regex = r\"([^\\/]+)(?=.all)\"\n",
    "matches = re.search(regex, test)\n",
    "new_file = matches.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_new = dt.datetime.fromtimestamp(test).date()\n",
    "type(test_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import datetime as dt\n",
    "import utils\n",
    "\n",
    "def clean_submissions(raw_submissions):\n",
    "    \"\"\"Cleans the file passed in.\n",
    "    Saves files to processed/submissions folder within trpred.\"\"\"\n",
    "\n",
    "    # Get name for processed file\n",
    "    regex = r\"([^\\/]+)(?=.allsubmissions)\"\n",
    "    matches = re.search(regex, raw_submissions)\n",
    "    new_file = matches.group(1)\n",
    "\n",
    "    # Create list of columns to keep\n",
    "    keep_cols = ['id', 'created_utc', 'author', 'title',\\\n",
    "                  'score', 'num_comments', 'subreddit', 'link_flair_text']\n",
    "\n",
    "    keep_cols_text = ['id', 'created_utc', 'author', 'selftext']\n",
    "\n",
    "    # Create file name\n",
    "    processedfile_csv = \"data/processed/submissions/\" + new_file + \\\n",
    "        \"-metadata\" +  \".csv\"\n",
    "\n",
    "    processed_textfile_csv = \"data/processed/submissions/\" + new_file + \\\n",
    "        \"-text\" + \".csv\"\n",
    "\n",
    "    # Create empty data frame\n",
    "    df_keep = pd.DataFrame()\n",
    "    df_keep_text = pd.DataFrame()\n",
    "\n",
    "    # Read in json file\n",
    "    try:\n",
    "        data = pd.read_json(raw_submissions)\n",
    "\n",
    "    # ValueError: Trailing data thrown if file is pretty indented\n",
    "    except ValueError:\n",
    "        data = pd.read_json(raw_submissions, lines = True)\n",
    "\n",
    "    try:\n",
    "        df_keep = df_keep.append(data[keep_cols])\n",
    "    except KeyError:\n",
    "        keep_cols = ['id', 'created_utc', 'author', 'title',\\\n",
    "                    'score', 'num_comments', 'subreddit']\n",
    "        df_keep = df_keep.append(data[keep_cols])\n",
    "\n",
    "    try:\n",
    "        df_keep_text = df_keep_text.append(data[keep_cols_text])\n",
    "    except KeyError:\n",
    "        keep_cols_text = ['id', 'created_utc', 'author']\n",
    "        df_keep_text = df_keep_text.append(data[keep_cols_text])\n",
    "\n",
    "\n",
    "    # Change date format\n",
    "    ## For metadata\n",
    "    df_keep['datetime_dv'] = pd.to_datetime(df_keep['created_utc'], unit = 's')# dv = derived\n",
    "    df_keep['date_dv'] = df_keep['datetime_dv'].dt.date\n",
    "\n",
    "    # For text\n",
    "    df_keep_text['datetime_dv'] = pd.to_datetime(df_keep_text['created_utc'], unit = 's')# dv = derived\n",
    "    df_keep_text['date_dv'] = df_keep_text['datetime_dv'].dt.date\n",
    "\n",
    "\n",
    "    ##### Delimit by date #####\n",
    "    start = dt.datetime.fromtimestamp(utils.get_startdate()).date()\n",
    "    end = dt.datetime.fromtimestamp(utils.get_enddate()).date()\n",
    "\n",
    "    # Create mask of time slot\n",
    "    mask = (df_keep['date_dv'] >= start) & (df_keep['date_dv'] <= end) # inclusive on either end\n",
    "    mask_text = (df_keep_text['date_dv'] >= start) & (df_keep_text['date_dv'] <= end)\n",
    "\n",
    "    # Only keep data within date frame\n",
    "    df_keep = df_keep.loc[mask]\n",
    "    df_keep_text = df_keep_text.loc[mask_text]\n",
    "    ############################\n",
    "\n",
    "\n",
    "    # Save to json\n",
    "    df_keep_text.to_csv(processed_textfile_csv, mode = \"w\")\n",
    "    df_keep.to_csv(processedfile_csv, mode = \"w\") # mode= w will overwrite previous file\n",
    "\n",
    "\n",
    "    data = [] # force empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "os.chdir(\"/Volumes/SAMSUNG/trpred\")\n",
    "\n",
    "\n",
    "processed_subs = glob.glob(\"data/processed/submissions/*.csv\")\n",
    "\n",
    "all_subs = glob.glob(\"data/raw/submissions/*.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "import os\n",
    "\n",
    "clean_processed = []\n",
    "\n",
    "for i in processed_subs:\n",
    "    regex = r\"([^\\/]+)(?=\\-)\" # set regex to get file names\n",
    "    matches = re.search(regex, i)\n",
    "    filename = matches.group()\n",
    "    clean_processed.append(filename)\n",
    " \n",
    "clean_processed = set(clean_processed)\n",
    "\n",
    "all_clean = []\n",
    "for i in all_subs:\n",
    "    regex = r\"([^\\/]+)(?=\\-all)\" # set regex to get file names\n",
    "    matches = re.search(regex, i)\n",
    "    filename = matches.group()\n",
    "    all_clean.append(filename)\n",
    "\n",
    "\n",
    "all_clean = set(all_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/raw/submissions/badwomensanatomy-allsubmissions-2020-08-13.json'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remaining = [x for x in all_clean if x not in clean_processed]\n",
    "\n",
    "remaining_files = ['data/raw/submissions/theredpillright-allsubmissions-2020-08-13.json',\n",
    "                   'data/raw/submissions/askseddit-allsubmissions-2020-08-13.json',\n",
    "                   'data/raw/submissions/thankTRP-allsubmissions-2020-08-13.json',\n",
    "                   'data/raw/submissions/redpillfatherhood-allsubmissions-2020-08-13.json',\n",
    "                   'data/raw/submissions/redpillbooks-allsubmissions-2020-08-13.json',\n",
    "                   'data/raw/submissions/mgtowbooks-allsubmissions-2020-08-19.json',\n",
    "                   'data/raw/submissions/mensrightslaw-allsubmissions-2020-08-15.json',\n",
    "                   'data/raw/submissions/seduction-allsubmissions-2020-08-23.json',\n",
    "                   'data/raw/submissions/askTRP-allsubmissions-2020-08-13.json',\n",
    "                   'data/raw/submissions/marriedredpill-allsubmissions-2020-08-13.json',\n",
    "                   'data/raw/submissions/exredpill-allsubmissions-2020-08-13.json',\n",
    "                   'data/raw/submissions/becomeaman-allsubmissions-2020-08-13.json',\n",
    "                   'data/raw/submissions/pua-allsubmissions-2020-08-19.json',\n",
    "                   'data/raw/submissions/theRedPillLeft-allsubmissions-2020-08-13.json',\n",
    "                   'data/raw/submissions/masculism-allsubmissions-2020-08-14.json',\n",
    "                   'data/raw/submissions/badwomensanatomy-allsubmissions-2020-08-13.json']\n",
    "\n",
    "remaining_files[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/raw/submissions/badwomensanatomy-allsubmissions-2020-08-13.json done\n"
     ]
    }
   ],
   "source": [
    "os.chdir(\"/Volumes/SAMSUNG/trpred\")\n",
    "\n",
    "for i in remaining_files[-1:]:\n",
    "    clean_submissions(i)\n",
    "    print(i + \" done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100sets\n",
      "100sets\n",
      "AskFeminists\n",
      "AskFeminists\n",
      "BlackPillScience\n",
      "BlackPillScience\n",
      "Braincels\n",
      "Braincels\n",
      "Egalitarianism\n",
      "Egalitarianism\n",
      "EthnicRedPill\n",
      "EthnicRedPill\n",
      "FeMRADebates\n",
      "FeMRADebates\n",
      "GEOTRP\n",
      "GEOTRP\n",
      "IncelTears\n",
      "IncelTears\n",
      "IncelsInAction\n",
      "IncelsInAction\n",
      "IncelsWithoutHate\n",
      "IncelsWithoutHate\n",
      "KotakuInAction\n",
      "KotakuInAction\n",
      "LeftWingMaleAdvocates\n",
      "LeftWingMaleAdvocates\n",
      "MGTOW\n",
      "MGTOW\n",
      "MGTOW2\n",
      "MGTOW2\n",
      "MGTOWmusic\n",
      "MGTOWmusic\n",
      "MRActivism\n",
      "MRActivism\n",
      "MRAmemes\n",
      "MRAmemes\n",
      "MensRants\n",
      "MensRants\n",
      "MensRights\n",
      "MensRights\n",
      "MensRightsMeta\n",
      "MensRightsMeta\n",
      "NOMAAM\n",
      "NOMAAM\n",
      "PurplePillDebate\n",
      "PurplePillDebate\n",
      "RedPillLit\n",
      "RedPillLit\n",
      "RedPillNonMonogamy\n",
      "RedPillNonMonogamy\n",
      "RedPillParenting\n",
      "RedPillParenting\n",
      "RedPillReadingGroup\n",
      "RedPillReadingGroup\n",
      "RedPillScience\n",
      "RedPillScience\n",
      "RedPillWives\n",
      "RedPillWives\n",
      "RedPillWomen\n",
      "RedPillWomen\n",
      "RedPillWorkplace\n",
      "RedPillWorkplace\n",
      "TRPOffTopic\n",
      "TRPOffTopic\n",
      "TRPmemes\n",
      "TRPmemes\n",
      "TheRedPill\n",
      "TheRedPill\n",
      "Trufemcels\n",
      "Trufemcels\n",
      "TumblrInAction\n",
      "TumblrInAction\n",
      "WhereAreAllTheGoodMen\n",
      "WhereAreAllTheGoodMen\n",
      "altTRP\n",
      "altTRP\n",
      "askTRP\n",
      "askTRP\n",
      "askanincel\n",
      "askanincel\n",
      "askseddit\n",
      "askseddit\n",
      "becomeaman\n",
      "becomeaman\n",
      "exredpill\n",
      "exredpill\n",
      "fPUA\n",
      "fPUA\n",
      "fappeningdiscussion\n",
      "fappeningdiscussion\n",
      "marriedredpill\n",
      "marriedredpill\n",
      "masculism\n",
      "masculism\n",
      "mensrightslaw\n",
      "mensrightslaw\n",
      "mgtowbooks\n",
      "mgtowbooks\n",
      "pua\n",
      "pua\n",
      "redpillbooks\n",
      "redpillbooks\n",
      "redpillfatherhood\n",
      "redpillfatherhood\n",
      "seduction\n",
      "seduction\n",
      "thankTRP\n",
      "thankTRP\n",
      "theRedPillLeft\n",
      "theRedPillLeft\n",
      "theredpillright\n",
      "theredpillright\n"
     ]
    }
   ],
   "source": [
    "# missing one subreddit\n",
    "os.chdir(\"/Volumes/SAMSUNG/trpred\")\n",
    "\n",
    "cleaned = glob.glob(\"data/processed/submissions/*.csv\")\n",
    "\n",
    "cleaned_ls = []\n",
    "for i in cleaned:\n",
    "    regex = r\"([^\\/]+)(?=\\-)\" # set regex to get file names\n",
    "    matches = re.search(regex, i)\n",
    "    filename = matches.group()\n",
    "    cleaned_ls.append(filename) \n",
    "\n",
    "cleaned_ls = set(cleaned_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_ls\n",
    "\n",
    "missing_one = [x for x in all_clean if x not in cleaned_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['badwomensanatomy']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
