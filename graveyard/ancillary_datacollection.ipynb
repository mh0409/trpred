{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import datetime as dt\n",
    "from psaw import PushshiftAPI # https://github.com/dmarx/psaw\n",
    "from glob import glob\n",
    "import matplotlib\n",
    "import requests\n",
    "import praw\n",
    "import all_scrape\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'praw' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4fc66179be06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m reddit = praw.Reddit(client_id=\"M5lv3Bhf_x9TNw\", client_secret=\"qiJ5mtjQlkElsJUCn6k_kf50M7c\",\\\n\u001b[0m\u001b[1;32m      2\u001b[0m                      \u001b[0muser_agent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"python:script:v1 (by /u/thrownaway8yrsago)\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                     \u001b[0muser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"thrownaway8yrsago\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m                     password = \"yCzFsR7_Vu.Y\")\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'praw' is not defined"
     ]
    }
   ],
   "source": [
    "reddit = praw.Reddit(client_id=\"M5lv3Bhf_x9TNw\", client_secret=\"qiJ5mtjQlkElsJUCn6k_kf50M7c\",\\\n",
    "                     user_agent=\"python:script:v1 (by /u/thrownaway8yrsago)\",\\\n",
    "                    user = \"thrownaway8yrsago\",\\\n",
    "                    password = \"yCzFsR7_Vu.Y\")\n",
    "\n",
    "new_ancillary = ['u_TRP_Scepter',\\\n",
    "                 'RedPillLit',\\\n",
    "                 'asktrp',\\\n",
    "                 'RedPillNonMonogamy',\\\n",
    "                 'RedPillWives',\\\n",
    "                 'redpillfatherhood',\\\n",
    "                 'redpillbooks',\\\n",
    "                 'RedPillWorkplace',\\\n",
    "                 'theRedPillLeft',\\\n",
    "                 'TRPmemes',\\\n",
    "                 'theredpillright',\\\n",
    "                 'EthnicRedPill',\\\n",
    "                 'marriedredpill',\\\n",
    "                 'ThankTRP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asktrp\n",
      "10000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-3ff9f70327c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Get submissions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mall_scrape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl_subreddit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Admin/yahb/Turing_Institute/trpred/all_scrape.py\u001b[0m in \u001b[0;36mcrawl_subreddit\u001b[0;34m(subreddit, max_submissions)\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_submissions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# to track progress for big pulls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_submissions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubreddit\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"-submissions-asof-\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoday\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".json\"\u001b[0m \u001b[0;31m# create filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# write file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "## Data collection of new ancillary subreddits\n",
    "\n",
    "import all_scrape\n",
    "\n",
    "# Get date\n",
    "today = dt.datetime.utcnow().date()\n",
    "\n",
    "# new_ancillary is a list that contains that new subreddits to scrape\n",
    "\n",
    "## Submissions ##\n",
    "for s in new_ancillary[2:]: # started at index 2 (asktrp) bc of bug that was corrected\n",
    "    print(s)\n",
    "    # Get submissions\n",
    "    all_scrape.crawl_subreddit(s)\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_ancillary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-dc8512c59279>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_ancillary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'new_ancillary' is not defined"
     ]
    }
   ],
   "source": [
    "new_ancillary[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting sense of size for the candidate subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_subred = [\"RedPillWomen\",\\\n",
    "                \"askTRP\",\\\n",
    "                \"RedPillParenting\",\\\n",
    "                \"thankTRP\",\\\n",
    "                \"RedPillLit\",\\\n",
    "                \"becomeaman\",\\\n",
    "                \"altTRP\",\\\n",
    "                \"GEOTRP\",\\\n",
    "                \"TRPOffTopic\",\\\n",
    "#                  \"u_TRP_Scepter\",\\ # this is a user, not a subreddit\n",
    "                \"RedPillNonMonogamy\",\\\n",
    "                \"RedPillWives\",\\\n",
    "                \"redpillfatherhood\",\\\n",
    "                \"redpillbooks\",\\\n",
    "                \"RedPillWorkplace\",\\\n",
    "                \"theRedPillLeft\",\\\n",
    "                \"TRPmemes\",\\\n",
    "                \"theredpillright\",\\\n",
    "                \"EthnicRedPill\",\\\n",
    "                \"marriedredpill\",\\\n",
    "                \"AskFeminists\", \"askseddit\", \"badwomensanatomy\",\\\n",
    "                \"Egalitarianism\", \"exredpill\", \"FeMRADebates\",\\\n",
    "                \"GEOTRP\", \"IncelsInAction\", \"IncelsWithoutHate\",\\\n",
    "                \"masculism\", \"MensRants\", \"MensRights\", \"mensrightslaw\",\\\n",
    "                \"MensRightsMeta\", \"MGTOW\", \"mgtowbooks\",\"MRActivism\",\n",
    "                \"NOMAAM\", \"pua\", \"PurplePillDebate\", \"seduction\",\\\n",
    "                \"Trufemcels\", \"fPUA\", \"100sets\",\\\n",
    "                \"askanincel\", \"LeftWingMaleAdvocates\", \"MRAmemes\", \"RedPillReadingGroup\",\\\n",
    "                \"RedPillScience\", \"WhereAreAllTheGoodMen\", \"KotakuInAction\",\\\n",
    "                \"TumblrInAction\", \"fappeningdiscussion\", \"BlackPillScience\",\\\n",
    "                \"MGTOW2\", \"MGTOWmusic\", \"IncelTears\",\"Braincels\"]\n",
    "\n",
    "## These all came from Ella's list; \n",
    "# need to sort and add to a new google sheet with sources; \n",
    "# then, run code [actually ran code on aug 11 -- see if data is ok]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bound by dates of interest\n",
    "start = int(dt.datetime(2012,10,25,0,0).timestamp())\n",
    "end = int(dt.datetime(2018,9,2,0,0).timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Get count of submissions per subreddit above\n",
    "subred_posts = {}\n",
    "\n",
    "# Get number of comments of all time (sanity check)\n",
    "for s in all_subred:\n",
    "    url = \"https://api.pushshift.io/reddit/search/submission/\"\n",
    "    queries = {\"subreddit\": s,\\\n",
    "                \"size\": 0,\\\n",
    "                \"aggs\" : \"subreddit\",\\\n",
    "              \"before\": end,\\\n",
    "              \"after\": start} \n",
    "\n",
    "    r = requests.get(url, params = queries)\n",
    "\n",
    "    # Get count (sanity check)\n",
    "    try: \n",
    "        count_posts = r.json()[\"aggs\"][\"subreddit\"][0][\"doc_count\"]\n",
    "        subred_posts[s] = count_posts\n",
    "    except: \n",
    "        subred_posts[s] = np.nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'RedPillWomen'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subred_posts\n",
    "df_subred_posts = pd.DataFrame.from_dict(subred_posts, orient = \"index\", columns = [\"posts\"])\n",
    "df_subred_posts.index.name = 'subreddit'\n",
    "df_subred_posts.reset_index(inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_subred_posts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-db419e2468f4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_subred_posts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df_subred_posts' is not defined"
     ]
    }
   ],
   "source": [
    "df_subred_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get count of comments per subreddit above\n",
    "subred_comments = {}\n",
    "\n",
    "# Get number of comments in all time (sanity check)\n",
    "for s in all_subred:\n",
    "    url = \"https://api.pushshift.io/reddit/search/comment/\"\n",
    "    queries = {\"subreddit\": s,\\\n",
    "                \"size\": 0,\\\n",
    "                \"aggs\" : \"subreddit\",\\\n",
    "              \"before\": end,\\\n",
    "              \"after\": start} \n",
    "\n",
    "    r = requests.get(url, params = queries)\n",
    "\n",
    "    # Get count (sanity check)\n",
    "    count_comments = r.json() # [\"aggs\"][\"subreddit\"][0][\"doc_count\"]\n",
    "\n",
    "#     subred_comments[s] = count_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subred_comments\n",
    "df_subred_comments = pd.DataFrame.from_dict(subred_comments, orient = \"index\", columns = [\"comments\"])\n",
    "df_subred_comments.index.name = 'subreddit'\n",
    "df_subred_comments.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subred_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's left to collect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what I have collected so far\n",
    "from glob import glob \n",
    "import os\n",
    "import re\n",
    "\n",
    "# change wd to external hd\n",
    "os.chdir(\"/Volumes/SAMSUNG/trpred\")\n",
    "\n",
    "cleaned_subreds = glob(\"data/raw/submissions/*.json\")\n",
    "\n",
    "ls_raw = []\n",
    "\n",
    "for i in cleaned_subreds:\n",
    "    try:\n",
    "        regex = r\"^.*\\/([^-]*)-.*$\"\n",
    "        matches = re.search(regex, i)\n",
    "        subreddit = matches.group(1)\n",
    "    except:\n",
    "        regex = r\"([^\\/]+)(?=\\.csv$)\"\n",
    "        matches = re.search(regex, i)\n",
    "        subreddit = matches.group(1)\n",
    "        \n",
    "    \n",
    "    ls_raw.append(subreddit)\n",
    "    \n",
    "    \n",
    "ls_raw = list(dict.fromkeys(ls_raw))\n",
    "len(ls_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['LeftWingMaleAdvocates', 'MRAmemes', 'MGTOWmusic']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing = set(ls_raw) - set(ls_cleaned)\n",
    "missing = list(missing)\n",
    "missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See what I have collected so far\n",
    "from glob import glob \n",
    "import os\n",
    "import re\n",
    "\n",
    "# change wd to external hd\n",
    "os.chdir(\"/Volumes/SAMSUNG/trpred\")\n",
    "\n",
    "cleaned_subreds = glob(\"data/processed/submissions/*.csv\")\n",
    "\n",
    "ls_cleaned = []\n",
    "\n",
    "for i in cleaned_subreds:\n",
    "    try:\n",
    "        regex = r\"^.*\\/([^-]*)-.*$\"\n",
    "        matches = re.search(regex, i)\n",
    "        subreddit = matches.group(1)\n",
    "    except:\n",
    "        regex = r\"([^\\/]+)(?=\\.csv$)\"\n",
    "        matches = re.search(regex, i)\n",
    "        subreddit = matches.group(1)\n",
    "        \n",
    "    \n",
    "    ls_cleaned.append(subreddit)\n",
    "    \n",
    "    \n",
    "ls_cleaned = list(dict.fromkeys(ls_cleaned))\n",
    "len(ls_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "import os \n",
    "\n",
    "os.chdir(\"/Volumes/SAMSUNG/trpred\")\n",
    "\n",
    "subreddits = []\n",
    "\n",
    "with open('missing_submissions.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "            subreddits.append(row)\n",
    "\n",
    "# all_subreddits holds the final list of subreddits we expect to have      \n",
    "missing_subreddits = [x for x in subreddits[0]]\n",
    "len(missing_subreddits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TheRedPillAskFeminists\n",
      "RedPillScience\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "IncelTears\n",
      "1000\n",
      "BlackPillScience\n",
      "100sets\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "askanincel\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Server returned status code 502",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-734a455b2310>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmissing_subreddits\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Get submissions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mall_scrape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrawl_subreddit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Admin/yahb/Turing_Institute/trpred/all_scrape.py\u001b[0m in \u001b[0;36mcrawl_subreddit\u001b[0;34m(subreddit, max_submissions)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_submissions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_submissions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mcurrent_submissions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_pages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubreddit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_posttime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_submissions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Admin/yahb/Turing_Institute/trpred/all_scrape.py\u001b[0m in \u001b[0;36mget_pages\u001b[0;34m(subreddit, last_posttime)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# something wrong happened\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Server returned status code {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: Server returned status code 502"
     ]
    }
   ],
   "source": [
    "## Data collection of new ancillary subreddits\n",
    "import all_scrape\n",
    "os.chdir(\"/Volumes/SAMSUNG/trpred\")\n",
    "\n",
    "## Submissions ## -- GO BACK TO GET MGTOW2\n",
    "for s in missing_subreddits: \n",
    "    # Get submissions\n",
    "    all_scrape.crawl_subreddit(s)\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now look to see what comments are missing\n",
    "os.chdir(\"/Volumes/SAMSUNG/trpred\")\n",
    "\n",
    "subreddits = []\n",
    "\n",
    "# Load in all subreddits\n",
    "with open('subreddits.csv', newline='') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "            subreddits.append(row)\n",
    "\n",
    "# Subreddits hold the final list of subreddits we expect to have   \n",
    "all_subreddits = [x for x in subreddits[0]]\n",
    "len(all_subreddits)\n",
    "\n",
    "subreddit_names = [] # will hold name of all subreddits for which we have submissions\n",
    "\n",
    "# See what comments I have\n",
    "file_comments = glob(\"data/processed/comments/*metadata.csv\")\n",
    "file_comments = list(dict.fromkeys(file_comments))\n",
    "\n",
    "for i in file_comments:\n",
    "    # get name of subreddit from file name\n",
    "    regex = r\"^.*\\/([^-]*)-.*$\"\n",
    "    matches = re.search(regex, i)\n",
    "    subreddit = matches.group(1)\n",
    "    \n",
    "    subreddit_names.append(subreddit)\n",
    "\n",
    "\n",
    "comments_toprocess = list(set(all_subreddits)-set(subreddit_names))\n",
    "len(comments_toprocess)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MensRants',\n",
       " '100sets',\n",
       " 'Braincels',\n",
       " 'masculism',\n",
       " 'MensRightsMeta',\n",
       " 'exredpill',\n",
       " 'fPUA',\n",
       " 'askanincel',\n",
       " 'IncelsInAction',\n",
       " 'TheRedPill']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_comments = glob(\"data/raw/comments/*.json\")\n",
    "raw_comments = list(dict.fromkeys(raw_comments))\n",
    "\n",
    "raw_ls = []\n",
    "\n",
    "for i in raw_comments:\n",
    "    regex = r\"^.*\\/([^-]*)-.*$\"\n",
    "    matches = re.search(regex, i)\n",
    "    subreddit = matches.group(1)\n",
    "    raw_ls.append(subreddit)\n",
    "\n",
    "comments_tocollect = list(set(all_subreddits)-set(raw_ls))\n",
    "\n",
    "comments_tocollect\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Volumes/SAMSUNG/trpred\")\n",
    "\n",
    "for s in comments_tocollect:\n",
    "    # Get comments\n",
    "    all_scrape.crawl_comments(s)\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
