{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import datetime as dt\n",
    "from psaw import PushshiftAPI # https://github.com/dmarx/psaw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Just seeing how this API works...\n",
    "# api = PushshiftAPI()\n",
    "# gen = api.search_submissions(limit=100)\n",
    "# results = list(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "## Submissions\n",
    "def crawl_page(subreddit: str, last_posttime = None):\n",
    "    \"\"\"Crawl a page of results from a given subreddit.\n",
    "    :param subreddit: The subreddit to crawl.\n",
    "    :param last_page: The last downloaded page.\n",
    "    :return: A page of results.\n",
    "    \"\"\"\n",
    "    \n",
    "    url = \"https://api.pushshift.io/reddit/search/submission\"\n",
    "    \n",
    "    params = {\"subreddit\": subreddit,\\\n",
    "               \"size\": 500,\\\n",
    "               \"sort\": \"desc\",\\\n",
    "               \"sort_type\": \"created_utc\"}\n",
    "    \n",
    "    # Called to \"scroll down\" page based on before\n",
    "    if last_posttime is not None:\n",
    "        queries[\"before\"] = last_posttime\n",
    "        \n",
    "    results = requests.get(url, params)\n",
    "    \n",
    "    if not results.ok:\n",
    "        # something wrong happened\n",
    "        raise Exception(\"Server returned status code {}\".format(results.status_code))\n",
    "    return results.json()[\"data\"]\n",
    "\n",
    "\n",
    "def crawl_subreddit(subreddit, max_submissions = 200000):\n",
    "    \"\"\"Crawl submissions from a subreddit.\n",
    "    :param subreddit: The subreddit to crawl.\n",
    "    :param max_submissions: The maximum number of submissions to download.\n",
    "    :return: A list of submissions.\"\"\"\n",
    "    \n",
    "    all_submissions = [] # empty list to hold all submissions\n",
    "    last_posttime = None  # will become an empty list when reached the last page\n",
    "    \n",
    "    while len(all_submissions) < max_submissions:\n",
    "        current_submissions = get_pages(subreddit, last_posttime)\n",
    "        if len(current_submissions) == 0:\n",
    "            break\n",
    "        last_posttime = current_submissions[-1][\"created_utc\"]\n",
    "        all_submissions += current_submissions\n",
    "        \n",
    "        #time.sleep(3)\n",
    "        \n",
    "        if len(all_submissions) % 10000 == 0: # to track progress for big pulls\n",
    "            print(len(all_submissions))\n",
    "    return all_submissions[:max_submissions]\n",
    "\n",
    "## Comments\n",
    "def crawl_comments(subreddit, max_comments = 10000000):\n",
    "    \"\"\"Crawl comments from a subreddit\n",
    "    :param subreddit: The subreddit to crawl.\n",
    "    :param max_submissions: The max number of comments to download.\n",
    "    :return: a data frame of comments\"\"\" \n",
    "    \n",
    "    api = PushshiftAPI()\n",
    "\n",
    "    gen = api.search_comments(subreddit = subreddit)\n",
    "    \n",
    "    comments = []\n",
    "    \n",
    "    for c in gen:\n",
    "        comments.append(c)\n",
    "        \n",
    "        if len(comments) % 10000 == 0:\n",
    "            print(len(comments))\n",
    "         # Omit this to not limit to max_comments\n",
    "#         if len(comments) >= max_comments:\n",
    "#             break\n",
    "    \n",
    "    # Below code only used if the `if len(comments)` lines above not commented out\n",
    "    if False: # False flag - to be changed to True if we want to get rest of the results\n",
    "        for c in gen:\n",
    "            comments.append(c)\n",
    "            \n",
    "    # Create pandas data frame to return        \n",
    "    df = pd.DataFrame([obj.d_ for obj in comments])\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<function BufferedWriter.close>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### SUBMISSIONS ### \n",
    "\n",
    "api = PushshiftAPI() \n",
    "\n",
    "url = \"https://api.pushshift.io/reddit/search/submission\"\n",
    "\n",
    "# Get number of submissions in entire subreddit\n",
    "# requests.get(url, params = {\"subreddit\": \"TheRedPill\", \"size\": 0, \"aggs\" : \"subreddit\"}).json()[\"aggs\"]\n",
    "# Result (May 15th): 112,196 posts in the entire subreddit\n",
    "\n",
    "\n",
    "# Get submissions\n",
    "submissions = crawl_subreddit(\"TheRedPill\")\n",
    "\n",
    "# Get date of submissions\n",
    "yesterday = today - dt.timedelta(days = 1) # will count/collect posts after 00:00 on this date\n",
    "\n",
    "# Save data as .json\n",
    "os.chdir(\"/Users/mariajoseherrera/Documents/Admin/yahb/Turing Institute/trpred/data/raw/submissions\")# change wd\n",
    "filename = \"submissions-\" + str(yesterday) + \".json\" # create filename \n",
    "\n",
    "with open(filename, 'w', encoding='utf-8') as f: # write file\n",
    "    json.dump(submissions, f, ensure_ascii = False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112207"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(submissions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COMMENTS ###\n",
    "\n",
    "# Get number of comments in entire subreddit\n",
    "#requests.get(url, params = {\"subreddit\": \"TheRedPill\", \"size\": 0, \"aggs\" : \"subreddit\"}).json()[\"aggs\"]\n",
    "# Result (May 15th): 3022067\n",
    "\n",
    "# Get comments\n",
    "df_comments = crawl_comments('TheRedPill')\n",
    "\n",
    "# Save data as .json\n",
    "os.chdir(\"/Users/mariajoseherrera/Documents/Admin/yahb/Turing Institute/trpred/data/raw/comments\")# change wd\n",
    "filename = \"comments-\" + str(yesterday) + \".json\" # create filename \n",
    "\n",
    "comments.to_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## ORIGINAL CODE FROM K.Ren (dissertation in progress)\n",
    "# import pandas as pd\n",
    "# import requests\n",
    "# import json\n",
    "# import csv\n",
    "# import time\n",
    "# import datetime as dt\n",
    "# from psaw import PushshiftAPI\n",
    "\n",
    "# url = \"https://api.pushshift.io/reddit/search/submission\"\n",
    "# params = {\"subreddit\": \"depressed\"}\n",
    "# submissions = requests.get(url, params = params)\n",
    "\n",
    "# api = PushshiftAPI()\n",
    "\n",
    "# gen = api.search_submissions(limit=100)\n",
    "# results = list(gen)\n",
    "\n",
    "# def crawl_page(subreddit: str, last_page = None):\n",
    "#     \"\"\"Crawl a page of results from a given subreddit.\n",
    "#     :param subreddit: The subreddit to crawl.\n",
    "#     :param last_page: The last downloaded page.\n",
    "#     :return: A page or results.\n",
    "#     \"\"\"\n",
    "#     params = {\"subreddit\": subreddit, \"size\": 500, \"sort\": \"desc\", \"sort_type\": \"created_utc\"}\n",
    "#     if last_page is not None:\n",
    "#         if len(last_page) > 0:\n",
    "#             # resume from where we left at the last page\n",
    "#             params[\"before\"] = last_page[-1][\"created_utc\"]\n",
    "#         else:\n",
    "#             # the last page was empty, we are past the last page\n",
    "#             return []\n",
    "#     results = requests.get(url, params)\n",
    "#     if not results.ok:\n",
    "#         # something wrong happened\n",
    "#         raise Exception(\"Server returned status code {}\".format(results.status_code))\n",
    "#     return results.json()[\"data\"]\n",
    "\n",
    "# def crawl_subreddit(subreddit, max_submissions = 200000):\n",
    "#     \"\"\"Crawl submissions from a subreddit.\n",
    "#     :param subreddit: The subreddit to crawl.\n",
    "#     :param max_submissions: The maximum number of submissions to download.\n",
    "#     :return: A list of submissions.\n",
    "#     \"\"\"\n",
    "#     submissions = []\n",
    "#     last_page = None\n",
    "#     while last_page != [] and len(submissions) < max_submissions:\n",
    "#         last_page = crawl_page(subreddit, last_page)\n",
    "#         submissions += last_page\n",
    "#         #time.sleep(3)\n",
    "#         if len(submissions) % 10000 == 0:\n",
    "#             print(len(submissions))\n",
    "#     return submissions[:max_submissions]\n",
    "\n",
    "# def crawl_comments(subreddit, max_comments = 10000000):\n",
    "    \n",
    "#     gen = api.search_comments(subreddit=subreddit)\n",
    "#     comments = []\n",
    "#     for c in gen:\n",
    "#         comments.append(c)\n",
    "        \n",
    "#         if len(comments) % 10000 == 0:\n",
    "#             print(len(comments))\n",
    "# #         # Omit this \n",
    "# #         if len(comments) >= max_comments:\n",
    "# #             break\n",
    "            \n",
    "#     # If you really want to: pick up where we left off to get the rest of the results.\n",
    "#     if False:\n",
    "#         for c in gen:\n",
    "#             comments.append(c)\n",
    "#     df = pd.DataFrame([obj.d_ for obj in comments])\n",
    "#     return df\n",
    "\n",
    "\n",
    "# requests.get(url, params = {\"subreddit\": \"SuicideWatch\", \"size\": 0, \"aggs\" : \"subreddit\"}).json()[\"aggs\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other sources:\n",
    "# source: https://medium.com/@RareLoot/using-pushshifts-api-to-extract-reddit-submissions-fb517b286563\n",
    "# source: https://medium.com/@pasdan/how-to-scrap-reddit-using-pushshift-io-via-python-a3ebcc9b83f4\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
