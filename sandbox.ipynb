{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code sandbox"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automation code (daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import time\n",
    "import datetime as dt\n",
    "from psaw import PushshiftAPI # https://github.com/dmarx/psaw\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate yesterday\n",
    "\n",
    "today = dt.datetime.utcnow().date()\n",
    "\n",
    "yesterday = today - dt.timedelta(days=1) # will count/collect posts after 00:00 on this date\n",
    "\n",
    "# print(yesterday) # sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submissions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Submissions code\n",
    "def get_pages(subreddit: str, last_posttime = None):\n",
    "    \"\"\"Crawl a page of results from a given subreddit over the past day.\n",
    "    :param subreddit: The subreddit to crawl.\n",
    "    :param last_posttime: The last downloaded page.\n",
    "    :return: A page of results.\n",
    "    \"\"\"\n",
    "    url = \"https://api.pushshift.io/reddit/search/submission\"\n",
    "    \n",
    "    # Calculate time window for past day\n",
    "    today = dt.datetime.utcnow().date()\n",
    "    yesterday = today - dt.timedelta(days = 1) # will count/collect posts after 00:00 on this date\n",
    "    \n",
    "    queries = {\"subreddit\": subreddit,\\\n",
    "               \"size\": 500,\\\n",
    "               \"sort\": \"desc\",\\\n",
    "               \"sort_type\": \"created_utc\",\\\n",
    "               \"before\": today,\\\n",
    "               \"after\": yesterday} \n",
    "    \n",
    "    # Called to \"scroll down\" page based on before\n",
    "    if last_posttime is not None:\n",
    "        queries[\"before\"] = last_posttime\n",
    "    \n",
    "    # Request data\n",
    "    results = requests.get(url, params = queries)\n",
    "    \n",
    "    # Check for errors\n",
    "    if not results.ok:\n",
    "        # something wrong happened\n",
    "        raise Exception(\"Server returned status code {}\".format(results.status_code))\n",
    "    \n",
    "    return results.json()[\"data\"]\n",
    "\n",
    "\n",
    "def get_dailysubmissions(subreddit, max_submissions = 200000):\n",
    "    \"\"\"Crawl submissions from a subreddit over past day (hard coded in `get_pages()`).\n",
    "    :param subreddit: The subreddit to crawl.\n",
    "    :param max_submissions: The maximum number of submissions to download.\n",
    "    :return: A list of submissions.\"\"\"\n",
    "    \n",
    "    all_submissions = [] # empty list to hold all submissions\n",
    "    last_posttime = None # will become an empty list when reached the last page\n",
    "    \n",
    "    while len(all_submissions) < max_submissions:\n",
    "        current_submissions = get_pages(subreddit, last_posttime)\n",
    "        if len(current_submissions) == 0:\n",
    "            break\n",
    "        last_posttime = current_submissions[-1][\"created_utc\"]\n",
    "        all_submissions += current_submissions\n",
    "        #time.sleep(3)\n",
    "        if len(all_submissions) % 10000 == 0: # to track progress for big pulls\n",
    "            print(len(all_submissions))\n",
    "    return all_submissions[:max_submissions]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test submissions\n",
    "submissions = get_dailysubmissions(\"TheRedPill\")\n",
    "\n",
    "# Get date of submissions\n",
    "yesterday = today - dt.timedelta(days = 1) # will count/collect posts after 00:00 on this date\n",
    "\n",
    "# Save data as .json\n",
    "os.chdir(\"/Users/mariajoseherrera/Documents/Admin/yahb/Turing Institute/trpred/data/raw/submissions\")# change wd\n",
    "filename = \"submissions-\" + str(yesterday) + \".json\" # create filename \n",
    "\n",
    "with open(filename, 'w', encoding='utf-8') as f: # write file\n",
    "    json.dump(submissions, f, ensure_ascii = False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dailycomments(subreddit, before, after, max_comments = 10000000):\n",
    "    \"\"\"Get comments from a subreddit over the past day\n",
    "    :param subreddit: The subreddit to crawl.\n",
    "    :param max_submissions: The max number of comments to download.\n",
    "    :return: a data frame of comments\"\"\" \n",
    "    \n",
    "    # Create instance of API\n",
    "    api = PushshiftAPI() \n",
    "    \n",
    "    # Using psaw package to access comments\n",
    "    gen = api.search_comments(subreddit = subreddit,\\\n",
    "                              before = today,\\\n",
    "                             after = yesterday)    \n",
    "    \n",
    "    # Create empty container for comments\n",
    "    comments = [] \n",
    "    \n",
    "    for c in gen:\n",
    "        comments.append(c) # append each item in generator\n",
    "        \n",
    "        if len(comments) % 10000 == 0: # track progress for large pulls\n",
    "            print(len(comments))\n",
    "         \n",
    "        # Omit this to not limit to max_comments\n",
    "#        if len(comments) >= max_comments:\n",
    "#             break\n",
    "    \n",
    "    # Below code only used if the `if len(comments)` lines above not commented out\n",
    "    if False: # False flag - to be changed to True if we want to get rest of the results\n",
    "        for c in gen:\n",
    "            comments.append(c)\n",
    "     \n",
    "    \n",
    "    # Create pandas data frame containing comments to return        \n",
    "    df = pd.DataFrame([obj.d_ for obj in comments])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of comments in past day (sanity check)\n",
    "url = \"https://api.pushshift.io/reddit/search/comment/\"\n",
    "queries = {\"subreddit\": \"TheRedPill\",\\\n",
    "            \"size\": 0,\\\n",
    "            \"aggs\" : \"subreddit\",\\\n",
    "            \"after\": yesterday,\\\n",
    "          \"before\": today} \n",
    "\n",
    "r = requests.get(url, params = queries)\n",
    "\n",
    "# Get count (sanity check)\n",
    "r.json()[\"aggs\"][\"subreddit\"][0][\"doc_count\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate time window\n",
    "today = dt.datetime.utcnow().date()\n",
    "yesterday = today - dt.timedelta(days = 1) # will count/collect posts after 00:00 on this date\n",
    "\n",
    "\n",
    "# Pull past day's comments (output: pandas df)\n",
    "comments = get_dailycomments(\"TheRedPill\", today, yesterday)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data as .json\n",
    "os.chdir(\"/Users/mariajoseherrera/Documents/Admin/yahb/Turing Institute/trpred/data/raw/comments\")# change wd\n",
    "filename = \"comments-\" + str(yesterday) + \".json\" # create filename \n",
    "\n",
    "comments.to_json(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
